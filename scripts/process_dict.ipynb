{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit0ecf791bd83b4b4eb3b96ac531fee81e",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dictionary on Phoenix dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def build_dictionary(file):\n",
    "    start = time.time()\n",
    "    lang_model = spacy.load('de_core_news_sm')\n",
    "    end = time.time()\n",
    "    print('load lang_model cost %.3f s'%(end-start))\n",
    "    train = []\n",
    "    # 合并annotation中的语料\n",
    "    df = pd.read_csv(file,sep='|')\n",
    "    for i in range(len(df)):\n",
    "        train.append(df.loc[i]['annotation'])\n",
    "\n",
    "    # Create a dictionary which maps tokens to indices (train contains all the training sentences)\n",
    "    freq_list = Counter()\n",
    "    punctuation = ['_','NULL','ON','OFF','EMOTION','LEFTHAND','IX','PU']\n",
    "    for sentence in train:\n",
    "        sentence = [tok.text for tok in lang_model.tokenizer(sentence) if not tok.text in punctuation]\n",
    "        freq_list.update(sentence)\n",
    "\n",
    "    # 按照词的出现频率建立词典，词频越高索引越靠前\n",
    "    freq_list = sorted(freq_list.items(),key=lambda item:item[1],reverse=True)\n",
    "    dictionary = {}\n",
    "    dictionary['<pad>'] = 0\n",
    "    dictionary['<bos>'] = 1\n",
    "    dictionary['<eos>'] = 2\n",
    "    dictionary['<unk>'] = 3\n",
    "    count = 0\n",
    "    for i,item in enumerate(freq_list):\n",
    "        if item[1] > 2:\n",
    "            dictionary[item[0]] = count+4\n",
    "            count += 1\n",
    "        else:\n",
    "            dictionary[item[0]] = 3\n",
    "    print(\"Build dictionary successfully!\")\n",
    "    return dictionary\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    punctuation = ['_','NULL','ON','OFF','EMOTION','LEFTHAND','IX','PU']\n",
    "    sentence = [tok.text for tok in lang_model.tokenizer(sentence) \n",
    "        if not tok.text in punctuation]\n",
    "    # sentence = ['<bos>'] +   sentence + ['<eos>']\n",
    "    print(sentence)\n",
    "    indices = [dictionary[word] for word in sentence \n",
    "        if word in dictionary.keys()]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load lang_model cost 1.076 s\nBuild dictionary successfully!\n"
    }
   ],
   "source": [
    "train_file = \"corpus/train.corpus.csv\"\n",
    "dictionary = build_dictionary(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find OOV words"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load lang_model cost 1.262 s\nBuild dictionary successfully!\nload lang_model cost 0.955 s\nBuild dictionary successfully!\nload lang_model cost 0.971 s\nBuild dictionary successfully!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[' ',\n 'E+R+Z',\n 'NICHT-WOLKE',\n 'S+H',\n 'C+M',\n 'HARZ',\n 'SECHZIG',\n 'HINREICHEND',\n 'MEIN',\n 'SICHT',\n 'RHEIN-PFALZ',\n 'WOANDERS',\n 'K+U+N+A',\n 'MITBRINGEN',\n 'ZWEIHUNDERT',\n 'WITTERUNG',\n 'ZEHN-STUNDEN',\n 'RUHIGER',\n 'NASE',\n 'HEINS',\n 'SECHSZEHNTE',\n 'ORIENTIEREN',\n 'EINHALB',\n 'NICHT-MOEGEN',\n 'STUFENWEISE',\n 'WEISS',\n 'GAENSEFUSS',\n 'ZWEITAUSEND',\n 'UEBERSPRINGEN',\n 'SEIN',\n 'MAL-SO',\n 'MANNHEIM',\n 'SLOWAKEI',\n 'BEIDE',\n 'MOECHTEN',\n 'ENTTAEUSCHT',\n 'KREISEN',\n 'MENSCHEN',\n 'FROH',\n 'KNOSPE-ABFALLEN',\n 'ERNTE',\n 'O+P+H+E+L+I+A',\n 'ABKUEHLEN',\n 'POSITION',\n 'BILD',\n 'NICHT-WAHRSCHEINLICH',\n 'ANKLICKEN',\n 'AB-SO',\n 'VERLAENGERN',\n 'VESCHWINDEN',\n 'VON-UNTEN',\n 'NACH-NORD',\n 'S+L+Y',\n 'DAS-WARS',\n 'TRINKEN',\n 'SELBST',\n 'ENTSCHEIDUNG',\n 'NICHT-IN-KOMMEND',\n 'SAEGE',\n 'GARTEN',\n 'AUTOBAHN',\n 'ZWISCHEN-MITTE',\n 'NEBEN',\n 'BESTE',\n 'SCHNEEVERWEHUNG']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train_file = '/Users/liweijie/SLR/scripts/corpus/train.corpus.csv'\n",
    "train_dict = build_dictionary([train_file])\n",
    "dev_file = '/Users/liweijie/SLR/scripts/corpus/dev.corpus.csv'\n",
    "dev_dict = build_dictionary([dev_file])\n",
    "test_file = '/Users/liweijie/SLR/scripts/corpus/test.corpus.csv'\n",
    "test_dict = build_dictionary([test_file])\n",
    "out_of_vocab = []\n",
    "for k,v in dev_dict.items():\n",
    "    if not k in train_dict.keys():\n",
    "        out_of_vocab.append(k)\n",
    "for k,v in test_dict.items():\n",
    "    if not k in train_dict.keys() and not k in dev_dict.keys():\n",
    "        out_of_vocab.append(k)\n",
    "out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}